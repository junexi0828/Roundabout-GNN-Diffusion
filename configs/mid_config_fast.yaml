# MID Fast 설정 - 1분 내 빠른 검증용 (Colab)
# 성능보다 속도 우선! 개념 검증 및 디버깅용

# ============================================================================
# 모델 설정 (최소 크기)
# ============================================================================
model:
  name: "mid_fast"

  obs_steps: 30      # 유지
  pred_steps: 50     # 유지

  # ⚡⚡ 초소형 모델
  hidden_dim: 32     # 64 → 32 (계산량 75% 감소)

  # ⚡⚡ Diffusion 스텝 최소화 (1분 내 완료용)
  num_diffusion_steps: 10  # 50 → 10 (1분 내 완료)

  beta_start: 0.0001
  beta_end: 0.02

  # ⚡⚡ 최소 Transformer
  denoiser:
    num_layers: 1      # 2 → 1
    num_heads: 2       # 4 → 2
    dropout: 0.1

  # ⚡⚡ 최소 인코더
  encoder:
    type: "lstm"
    num_layers: 1
    use_transformer: false

  # ⚡⚡ GNN 비활성화 (속도 우선)
  use_gnn: false

# ============================================================================
# 데이터 설정 (최소 샘플)
# ============================================================================
data:
  data_dir: "data/processed"

  # ⚡⚡ 배치 크기 최대화 (1분 내 완료용)
  batch_size: 256    # 64 → 256 (더 빠른 처리)

  num_workers: 4     # 데이터 로딩 병렬화
  pin_memory: true   # GPU 전송 속도 향상

  # ⚡⚡ 극소량 데이터 (1% - 1분 내 완료용)
  use_sampling: true
  sample_ratio: 0.01  # 30% → 1% (1분 내 완료)

  train_ratio: 0.8    # 검증/테스트 줄이기
  val_ratio: 0.1
  test_ratio: 0.1

  augmentation:
    enabled: false

# ============================================================================
# 학습 설정 (초고속)
# ============================================================================
training:
  # ⚡⚡ 높은 학습률 (빠른 수렴)
  optimizer: "adamw"
  learning_rate: 0.001   # 0.0003 → 0.001 (3배 빠른 수렴)
  weight_decay: 0.0001
  betas: [0.9, 0.999]

  # ⚡⚡ Scheduler
  scheduler: "cosine"
  warmup_epochs: 1      # 3 → 1
  min_lr: 0.0001

  # ⚡⚡ 에폭 최소화 (1분 내 완료용)
  num_epochs: 1         # 50 → 1 (1분 내 완료)

  max_grad_norm: 1.0

  # ⚡⚡ 조기 종료 강화
  early_stopping:
    patience: 5         # 10 → 5
    min_delta: 0.01     # 0.001 → 0.01 (더 관대하게)

  loss:
    type: "mse"
    weight_position: 1.0

  # ⚡⚡ Mixed Precision 필수
  use_amp: true

  gradient_checkpointing: false

# ============================================================================
# 평가 설정 (최소화)
# ============================================================================
evaluation:
  metrics:
    - "ade"
    - "fde"

  # ⚡⚡ 최소 샘플링 (1분 내 완료용)
  num_samples: 3       # 5 → 3 (더 빠른 평가)
  ddim_steps: 1        # 2 → 1 (더 빠른 샘플링)
  ddim_eta: 0.0

  # ⚡⚡ 평가 최소화 (1분 내 완료용)
  eval_every: 1        # 매 에폭마다 (1 에폭만 실행)

# ============================================================================
# 로깅 (최소화)
# ============================================================================
logging:
  log_dir: "runs/mid_fast"
  save_dir: "checkpoints/mid_fast"

  # ⚡⚡ 저장 최소화 (1분 내 완료용)
  save_every: 1        # 매 에폭마다 저장 (1 에폭만)

  tensorboard: true
  log_images: false
  log_every: 500       # 200 → 500

  use_wandb: false

# ============================================================================
# 디바이스 설정
# ============================================================================
device:
  use_cuda: true
  device_id: 0
  multi_gpu: false
  gpu_ids: [0]

# ============================================================================
# Ultra Fast 모드 (1분 내 완료용)
# ============================================================================
ultra_fast_mode:
  enabled: true

  # ⚡⚡ 추가 최적화 (1분 내 완료용)
  skip_validation_until_epoch: 1   # 1 에폭까지 검증 스킵 (1 에폭만 실행하므로 사실상 스킵)
  max_train_batches: 10            # 학습 배치 10개만 (1분 내 완료)
  max_val_batches: 5               # 검증 배치 5개만 (1분 내 완료)
  cache_data: true                  # 데이터 캐싱
  deterministic: false              # 속도 우선

# ============================================================================
# 재현성
# ============================================================================
seed: 42
deterministic: false  # 속도 우선

# ============================================================================
# 예상 시간 (Colab T4 기준)
# ============================================================================
#
# Fast 모드 (1분 내 완료):
# - Colab (T4 GPU): 1분 이내 ⚡⚡⚡
#
# 성능: ~50-60% (디버깅/개념검증용)
#
# ============================================================================
